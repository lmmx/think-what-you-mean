{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49f3249d-4467-4b0f-8d0c-52c6a0139560",
   "metadata": {},
   "source": [
    "## GSM8K\n",
    "\n",
    "This notebook reproduces the results for the GSM8K evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33194d54-582b-4b81-a195-8f1269a438a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 7473\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1319\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import outlines\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from textwrap import dedent\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from outlines.samplers import greedy\n",
    "\n",
    "MODEL_NAME = \"casperhansen/deepseek-r1-distill-qwen-7b-awq\"\n",
    "# Load the dataset from HuggingFace\n",
    "dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "# You can inspect the dataset structure\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b196b250-d8a6-40ac-b2be-66014ca2548a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 7473\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31a054b5-3aba-4875-9bd1-a8dd54252934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1319\n"
     ]
    }
   ],
   "source": [
    "all_evals = list(dataset['test'])\n",
    "print(len(all_evals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95958a3a-7357-4181-b0b8-33771c286626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-28 01:23:43 config.py:510] This model supports multiple tasks: {'embed', 'score', 'generate', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 01-28 01:23:44 awq_marlin.py:109] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "WARNING 01-28 01:23:44 arg_utils.py:1103] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 01-28 01:23:44 config.py:1458] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 01-28 01:23:44 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='casperhansen/deepseek-r1-distill-qwen-7b-awq', speculative_config=None, tokenizer='casperhansen/deepseek-r1-distill-qwen-7b-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=casperhansen/deepseek-r1-distill-qwen-7b-awq, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 01-28 01:23:44 selector.py:120] Using Flash Attention backend.\n",
      "INFO 01-28 01:23:45 model_runner.py:1094] Starting to load model casperhansen/deepseek-r1-distill-qwen-7b-awq...\n",
      "INFO 01-28 01:23:45 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1af22d4f5d4ebdb0be6b534c43d5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-28 01:23:46 model_runner.py:1099] Loading model weights took 5.2282 GB\n",
      "INFO 01-28 01:23:47 worker.py:241] Memory profiling takes 0.74 seconds\n",
      "INFO 01-28 01:23:47 worker.py:241] the current vLLM instance can use total_gpu_memory (23.68GiB) x gpu_memory_utilization (0.90) = 21.31GiB\n",
      "INFO 01-28 01:23:47 worker.py:241] model weights take 5.23GiB; non_torch_memory takes 0.24GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 14.44GiB.\n",
      "INFO 01-28 01:23:47 gpu_executor.py:76] # GPU blocks: 16900, # CPU blocks: 4681\n",
      "INFO 01-28 01:23:47 gpu_executor.py:80] Maximum concurrency for 131072 tokens per request: 2.06x\n",
      "INFO 01-28 01:23:49 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:13<00:00,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-28 01:24:03 model_runner.py:1535] Graph capturing finished in 13 secs, took 0.26 GiB\n",
      "INFO 01-28 01:24:03 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 16.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, create_model\n",
    "from vllm import LLM, SamplingParams\n",
    "from outlines.models.vllm import adapt_tokenizer\n",
    "from outlines.processors import JSONLogitsProcessor\n",
    "\n",
    "llm = LLM(MODEL_NAME, enable_prefix_caching=True)\n",
    "tokenizer = llm.get_tokenizer()\n",
    "outlines_tokenizer = adapt_tokenizer(AutoTokenizer.from_pretrained(MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "531655e1-b4a3-4d83-8d22-be34d7667b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_question = [\n",
    "    \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",\n",
    "    \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",\n",
    "    \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a9ee758-437a-4fa2-b622-a0fe0423600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: `tokenizer.apply_chat_template` strips out the <think>...</think> when formatting messages, so these will be lost from the demos!\n",
    "example_response = [\n",
    "    \"\"\"<think>There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6.</think>{\"answer\": 6}\"\"\",\n",
    "    \"\"\"<think>There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5.</think>{\"answer\": 5}\"\"\",\n",
    "    \"\"\"<think>Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39.</think>{\"answer\": 39}\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd18c953-71eb-4f43-9a17-6c7d17c52e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it.\\nBefore answering you should reason about the problem (using the \"reasoning\" field in the JSON response described below).\\n\\nYou will always respond with a thinking step followed immediately by JSON in the format described below:\\n\\n```\\n<think>Reasoning about the answer goes here</think>{\"answer\": \"Final answer goes here\"}\\n```\\n\\nThe `<think>...</think>` section will contain your reasoning about the sequence of events.\\n\\nThe JSON \"answer\" will contain the integer representing the correct answer to the question.\\n'}, {'role': 'user', 'content': 'Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?'}, {'role': 'assistant', 'content': '<think>There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6.</think>{\"answer\": 6}'}, {'role': 'user', 'content': 'Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?'}, {'role': 'assistant', 'content': '<think>There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5.</think>{\"answer\": 5}'}, {'role': 'user', 'content': 'Question: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?'}, {'role': 'assistant', 'content': '<think>Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39.</think>{\"answer\": 39}'}]\n"
     ]
    }
   ],
   "source": [
    "# Note: the system message can contain the `<think>` and `</think>` tags and the tokenizer does not remove them when applying chat template\n",
    "prompt_icl_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": dedent(\"\"\"\\\n",
    "        You are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it.\n",
    "        Before answering you should reason about the problem (using the \"reasoning\" field in the JSON response described below).\n",
    "          \n",
    "        You will always respond with a thinking step followed immediately by JSON in the format described below:\n",
    "\n",
    "        ```\n",
    "        <think>Reasoning about the answer goes here</think>{\"answer\": \"Final answer goes here\"}\n",
    "        ```\n",
    "\n",
    "        The `<think>...</think>` section will contain your reasoning about the sequence of events.\n",
    "        \n",
    "        The JSON \"answer\" will contain the integer representing the correct answer to the question.\n",
    "        \"\"\")\n",
    "    },\n",
    "]\n",
    "for i, _ in enumerate(example_question):\n",
    "    prompt_icl_messages.extend(\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Question: {question}\".format(question=example_question[i])},\n",
    "            {\"role\": \"assistant\", \"content\": example_response[i]},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(prompt_icl_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c85cd043-5cca-4205-9cba-7903ecf4e4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it.\n",
      "Before answering you should reason about the problem (using the \"reasoning\" field in the JSON response described below).\n",
      "\n",
      "You will always respond with a thinking step followed immediately by JSON in the format described below:\n",
      "\n",
      "```\n",
      "<think>Reasoning about the answer goes here</think>{\"answer\": \"Final answer goes here\"}\n",
      "```\n",
      "\n",
      "The `<think>...</think>` section will contain your reasoning about the sequence of events.\n",
      "\n",
      "The JSON \"answer\" will contain the integer representing the correct answer to the question.\n",
      "<｜User｜>Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?<｜Assistant｜>{\"answer\": 6}<｜end▁of▁sentence｜><｜User｜>Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?<｜Assistant｜>{\"answer\": 5}<｜end▁of▁sentence｜><｜User｜>Question: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?<｜Assistant｜>{\"answer\": 39}<｜end▁of▁sentence｜><｜User｜>Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?<｜Assistant｜>\n"
     ]
    }
   ],
   "source": [
    "def create_prompt(question, tokenizer):\n",
    "    messages = list(prompt_icl_messages)\n",
    "    messages.extend(\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Question: {question}\".format(question=question)},\n",
    "        ]\n",
    "    )\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(create_prompt(all_evals[5]['question'], tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82efc86d-08ea-45d7-a3d4-efe84135f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, constr\n",
    "from outlines_core.fsm.json_schema import build_regex_from_schema\n",
    "import json\n",
    "\n",
    "class Response(BaseModel):\n",
    "    answer: int = Field(pattern=r'[1-9][0-9]{0,9}')\n",
    "\n",
    "\n",
    "schema_regex = build_regex_from_schema(json.dumps(Response.model_json_schema()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dd5ef47-38a5-48e2-8ccb-1f69f5226700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_evals[5]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fce2d5a-1d3d-4ea5-aa9c-0204548fbf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature = 0 means greedy sampling\n",
    "# https://docs.vllm.ai/en/stable/api/inference_params.html\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "def create_generator(llm: LLM, **sampling_kwargs):\n",
    "    base_params = sampling_kwargs\n",
    "    \n",
    "    def generator(prompt: str, max_tokens: int = 2048):\n",
    "        params = SamplingParams(\n",
    "            **{**base_params, 'max_tokens': max_tokens}\n",
    "        )\n",
    "        outputs = llm.generate([prompt], params)\n",
    "        return outputs[0].outputs[0].text\n",
    "    \n",
    "    return generator\n",
    "\n",
    "open_generator = create_generator(llm, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06ea5b16-63d6-46e2-8e1e-0d85eda09c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function create_generator.<locals>.generator at 0x745d4eb26340>\n"
     ]
    }
   ],
   "source": [
    "print(open_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54a68fb7-e641-4ec3-a1ae-dcfbbe28411f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.80s/it, est. speed input: 85.50 toks/s, output: 125.22 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, so Kylar wants to buy 16 glasses, and each glass normally costs $5. But every second glass is on sale for 60% of the original price. Hmm, I need to figure out how much he'll pay in total.\\n\\nFirst, I should understand the pricing structure. Every second glass is 60% off, which means it costs 60% of $5. Let me calculate that: 60% of $5 is 0.6 * 5 = $3. So, every even-numbered glass is $3, and the odd-numbered ones are $5.\\n\\nNow, Kylar is buying 16 glasses. Since every two glasses consist of one regular and one discounted, I can think of this as 8 sets of two glasses each. Each set has one $5 glass and one $3 glass.\\n\\nSo, for each set, the cost is $5 + $3 = $8. Since there are 8 sets, the total cost would be 8 * $8 = $64.\\n\\nWait, let me double-check. 16 glasses mean 8 regular and 8 discounted. 8 * $5 = $40, and 8 * $3 = $24. Adding those together: $40 + $24 = $64. Yep, that matches my earlier calculation.\\n\\nSo, Kylar needs to pay $64 in total.\\n</think>\\n\\nKylar needs to pay $64 for 16 glasses.\\n\\n**Step-by-Step Explanation:**\\n\\n1. **Determine the cost of every two glasses:**\\n   - First glass: $5\\n   - Second glass: 60% of $5 = $3\\n   \\n2. **Calculate the cost for one set of two glasses:**\\n   - $5 (first glass) + $3 (second glass) = $8\\n   \\n3. **Determine the number of sets in 16 glasses:**\\n   - 16 glasses ÷ 2 glasses/set = 8 sets\\n   \\n4. **Calculate the total cost for 8 sets:**\\n   - 8 sets × $8/set = $64\\n\\n**Answer:**  \\nKylar needs to pay $\\\\boxed{64}$ dollars.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_generator(create_prompt(all_evals[5]['question'], tokenizer),max_tokens=2560)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49a1e80d-ebf5-480f-b27f-0575573d1ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(879, 892), match='{\"answer\": 6}'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(schema_regex, create_prompt(all_evals[5]['question'], tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6979209b-a66e-43b1-b3a9-a47e36c17316",
   "metadata": {},
   "source": [
    "## Unstructured Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8a091ce-b273-44bc-9d7f-23de8cbdb8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAST = len(all_evals)\n",
    "answer_regex = r'\"answer\":[ ]?([1-9][0-9]{0,9})'\n",
    "answers = []\n",
    "for ex_eval in all_evals[0:LAST]:\n",
    "    raw_int = ex_eval['answer'].split('#### ')[1]\n",
    "    raw_int = re.sub(\",\",\"\",raw_int)\n",
    "    answers.append(int(raw_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a5625-62d4-4264-b3e0-17b63135fe6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating 1319 evals:   0%|                                                                                                                                                                                                         | 0/1319 [00:00<?, ?it/s]\n",
      "\u001b[Acessed prompts:   0%|                                                                                                                                                            | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.99s/it, est. speed input: 111.54 toks/s, output: 126.95 toks/s]\n",
      "Generating 1319 evals:   0%|▏                                                                                                                                                                                              | 1/1319 [00:02<1:05:39,  2.99s/it]\n",
      "\u001b[Acessed prompts:   0%|                                                                                                                                                            | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.73s/it, est. speed input: 108.01 toks/s, output: 128.15 toks/s]\n",
      "Generating 1319 evals:   0%|▎                                                                                                                                                                                              | 2/1319 [00:05<1:02:20,  2.84s/it]\n",
      "\u001b[Acessed prompts:   0%|                                                                                                                                                            | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.98s/it, est. speed input: 109.52 toks/s, output: 127.99 toks/s]\n",
      "Generating 1319 evals:   0%|▍                                                                                                                                                                                              | 3/1319 [00:08<1:03:42,  2.90s/it]\n",
      "\u001b[Acessed prompts:   0%|                                                                                                                                                            | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.12s/it, est. speed input: 143.73 toks/s, output: 127.65 toks/s]\n",
      "Generating 1319 evals:   0%|▌                                                                                                                                                                                                | 4/1319 [00:10<56:51,  2.59s/it]\n",
      "\u001b[Acessed prompts:   0%|                                                                                                                                                            | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.05s/it, est. speed input: 124.61 toks/s, output: 126.91 toks/s]\n",
      "Generating 1319 evals:   0%|▋                                                                                                                                                                                              | 5/1319 [00:13<1:00:26,  2.76s/it]\n",
      "\u001b[Acessed prompts:   0%|                                                                                                                                                            | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.73s/it, est. speed input: 87.10 toks/s, output: 127.57 toks/s]\n",
      "Generating 1319 evals:   0%|▊                                                                                                                                                                                              | 6/1319 [00:17<1:07:39,  3.09s/it]\n",
      "\u001b[Acessed prompts:   0%|                                                                                                                                                            | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.76s/it, est. speed input: 54.01 toks/s, output: 126.77 toks/s]\n",
      "Generating 1319 evals:   1%|█                                                                                                                                                                                              | 7/1319 [00:23<1:26:42,  3.97s/it]\n",
      "\u001b[Acessed prompts:   0%|                                                                                                                                                            | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.24s/it, est. speed input: 104.31 toks/s, output: 127.15 toks/s]\n",
      "Generating 1319 evals:   1%|█▏                                                                                                                                                                                             | 8/1319 [00:26<1:21:37,  3.74s/it]\n",
      "\u001b[Acessed prompts:   0%|                                                                                                                                                            | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.60s/it, est. speed input: 66.62 toks/s, output: 126.28 toks/s]\n",
      "Generating 1319 evals:   1%|█▎                                                                                                                                                                                             | 9/1319 [00:32<1:34:18,  4.32s/it]\n",
      "\u001b[Acessed prompts:   0%|                                                                                                                                                            | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    }
   ],
   "source": [
    "free_resp = []\n",
    "for i in tqdm(range(LAST), desc=f\"Generating {LAST} evals\"):\n",
    "    free_resp.append(open_generator(create_prompt(all_evals[i]['question'], tokenizer), max_tokens=2560))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ec11e-287b-4553-b276-8e5efcec42ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_resp[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8003f-fdd5-458b-89c9-0627f66a6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_resp[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fae3d2-99b6-4de1-bcd3-bf6b40428717",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_resp[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f978b4a4-8c4e-4e2b-97dc-0ce52ece58ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_resp_answers = [int(result[1].upper()) if result else \"\" for result in [re.search(answer_regex,resp) for resp in free_resp]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a5cf9c-4496-4919-802c-25f6e0bcf727",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(free_resp_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b132333-0217-4a45-a814-75208e6de990",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e653f0-6766-4d2f-9553-d48e2ff74d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean([result[0] == result[1] for result in zip(free_resp_answers, answers)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd59003-de09-4aa6-bfe5-1a8c0bf6474f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T19:17:29.562865Z",
     "iopub.status.busy": "2024-11-01T19:17:29.561917Z",
     "iopub.status.idle": "2024-11-01T19:17:29.570077Z",
     "shell.execute_reply": "2024-11-01T19:17:29.568594Z",
     "shell.execute_reply.started": "2024-11-01T19:17:29.562778Z"
    }
   },
   "source": [
    "## Structured Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f4b1db-145d-4bd9-b9c6-340376b51fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(schema_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57d9b4c-378b-444d-a8e1-955e267a9087",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot, eot = \"<think>\", \"</think>\"\n",
    "\n",
    "\n",
    "class StructuredAnswer(BaseModel):\n",
    "    answer: int\n",
    "    \n",
    "\n",
    "class TriggerBasedLogitsProcessor:\n",
    "    \"\"\"Logits processor that triggers JSON generation after </think> token\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, base_processor, guide: type[BaseModel] = StructuredAnswer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.base_processor = base_processor\n",
    "        self.guide = guide\n",
    "        self.bot_id, self.eot_id = tokenizer.convert_tokens_to_ids([bot, eot])\n",
    "        self.triggered = False\n",
    "        self.triggered_at = -1\n",
    "        self.in_cot = False\n",
    "        self.history = []\n",
    "        self.cot = \"\"\n",
    "\n",
    "    def __call__(\n",
    "        self, prompt: tuple[int], generated_tokens: tuple[int], logits: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        if not self.in_cot and not self.cot:\n",
    "            # We may be in the CoT if the BOT was in the prompt (but not EOT)\n",
    "            self.in_cot = self.bot_id in prompt and self.eot_id not in prompt\n",
    "            if self.in_cot:\n",
    "                self.cot = self.tokenizer.decode(\n",
    "                    prompt[prompt.index(self.bot_id) + 1 :]\n",
    "                ).lstrip()\n",
    "        if len(generated_tokens) > 0:\n",
    "            last_id = generated_tokens[-1]\n",
    "            if not self.in_cot:\n",
    "                self.in_cot = last_id == self.bot_id\n",
    "            if self.in_cot:\n",
    "                is_eot = last_id == self.eot_id\n",
    "                if is_eot:\n",
    "                    self.triggered_at = len(generated_tokens)\n",
    "                    self.triggered = True\n",
    "                    self.in_cot = False\n",
    "                    self.cot += self.tokenizer.decode(self.history).strip()\n",
    "            self.history.append(last_id)\n",
    "\n",
    "        # Only apply base processor if triggered\n",
    "        if self.triggered:\n",
    "            return self.base_processor(generated_tokens, logits)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e07900-54bc-4d4f-a2bc-3b8103884e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structured_generator = outlines.generate.regex(model, schema_regex, sampler=greedy())\n",
    "\n",
    "def structured_generator(prompt, guide = StructuredAnswer, temperature=0, max_new_tokens=2560):\n",
    "    json_schema = json.dumps(guide.model_json_schema())\n",
    "    model_name = llm.llm_engine.model_config.model\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    outlines_tokenizer = adapt_tokenizer(AutoTokenizer.from_pretrained(model_name))\n",
    "    guided_processor = JSONLogitsProcessor(\n",
    "        schema=json_schema, tokenizer=outlines_tokenizer, whitespace_pattern=r\" ?\"\n",
    "    )\n",
    "    conditional_guide_processor = TriggerBasedLogitsProcessor(\n",
    "        tokenizer=outlines_tokenizer, base_processor=guided_processor, guide=guide\n",
    "    )\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_new_tokens,\n",
    "        logits_processors=[conditional_guide_processor],\n",
    "    )\n",
    "    # Generate output\n",
    "    logits_processor = sampling_params.logits_processors[0]  # or conditional_guide_processor\n",
    "    output = llm.generate(prompt, sampling_params, use_tqdm=False)\n",
    "    generated_text = output[0].outputs[0].text\n",
    "    cot = logits_processor.cot\n",
    "    # JSON structured response here\n",
    "    post_cot = tokenizer.decode(\n",
    "        logits_processor.history[logits_processor.triggered_at :],\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    structured = (\n",
    "        '{\"reasoning\": \"' + json.dumps(cot)[1:-1] + '\", ' + post_cot.removeprefix(\"{\")\n",
    "    )\n",
    "    return structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f7601b-2490-42a0-851a-8be166ecc547",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_resp = [structured_generator(create_prompt(all_evals[i]['question'], tokenizer)) for i in tqdm(range(LAST))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e4048b-94c0-40de-9c7b-fa689cc4834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_resp[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134d296e-19b1-41ea-b2b4-33bcea8d0a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_resp_answers = [int(result[1].upper()) if result else \"\" for result in [re.search(answer_regex,resp) for resp in structured_resp]]\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6226335d-c3f0-42ef-86f7-de1ab8911bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean([result[0] == result[1] for result in zip(structured_resp_answers, answers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f80e9-0438-47d6-b446-f54744524cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,figsize=(10,8),facecolor='white')\n",
    "ax.bar('unstructured',np.mean([result[0] == result[1] for result in zip(free_resp_answers, answers)]),label='unstructured')\n",
    "ax.bar('structured',np.mean([result[0] == result[1] for result in zip(structured_resp_answers, answers)]),label='structured')\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.set_title(f\"GSM8K - Unstructured vs. JSON Structured\\n{MODEL_NAME}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
